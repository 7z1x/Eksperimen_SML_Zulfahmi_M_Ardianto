    # -*- coding: utf-8 -*-
    """Eksperimen_Zulfahmi_M_Ardianto.ipynb

    Automatically generated by Colab.

    Original file is located at
        https://colab.research.google.com/drive/1nPbPyeU1oECpvKd7X--NnlIZLp4I4z3W

    # Proyek Klasifikasi Kanker Payudara

    # **1. Perkenalan Dataset**

    Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:

    **Sumber Dataset**:  
    Sumber: Breast cancer prediction Dataset dari Kaggle dan bersumber dari Wisconsin breast cancer diagnostic dataset

    # **2. Import Library**

    Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning.
    """

    # Commented out IPython magic to ensure Python compatibility.
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    # %matplotlib inline
    import seaborn as sns
    # %matplotlib inline
    import matplotlib.gridspec as gridspec
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler

    from google.colab import files
    files.upload()  # upload kaggle.json kamu

    !mkdir -p ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json

    """# **3. Memuat Dataset**

    Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

    Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.

    Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan
    """

    # Download kaggle dataset and unzip the file
    # !cp kaggle.json ~/.kaggle/

    # !chmod 600 ~/.kaggle/kaggle.json
    !kaggle datasets download -d uciml/breast-cancer-wisconsin-data
    !unzip breast-cancer-wisconsin-data.zip

    df = pd.read_csv("/content/data.csv")
    df

    """# **4. Exploratory Data Analysis (EDA)**

    Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.

    Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.

    **Insight:**

    - Dataset memiliki 569 baris (pengamatan) dan 33 kolom (fitur + target).
    - Kolom diagnosis adalah target dengan nilai M (malignant) dan B (benign).
    - Kolom id tidak relevan untuk modeling, dan kolom Unnamed: 32 berisi nilai kosong.
    """

    df.head()

    """**Insight:**

    - Kode ini untuk melihat statistik deskriptif dari dataset Breast Cancer
    """

    df.info()

    df.describe()

    """# Cek Missing Value dan Duplicate

    """

    df.isna().sum()

    """**Insight:**

    - Unnamed: 32 memiliki 562 nilai kosong dan harus dilakukan tindakan
    """

    df.duplicated().sum()

    df.diagnosis.unique()

    """### Distribusi Data"""

    df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})
    df.head()

    """**Insight:**

    - melakukan label encoding pada data kategorikal. pengecualian ini dilakukan pada proses EDA karena untuk memudahkan melihat data distribusi **bukan karena salah tidak menaruhnya pada proses preprocessing**
    """

    # Plot distribusi target
    plt.figure(figsize=(6, 4))
    sns.countplot(x='diagnosis', data=df, palette='Set2')
    plt.title('Distribusi Diagnosis (1: Malignant, 0: Benign)')
    plt.xlabel('Diagnosis')
    plt.ylabel('Jumlah')
    plt.xticks([0, 1], ['Benign', 'Malignant'])
    plt.show()

    print(df['diagnosis'].value_counts())

    """**Insight:**

    - B memiliki data 357 dan M memiliki 212

    - Distribusi target menunjukkan lebih banyak kasus benign (63%) dibandingkan malignant (37%), tetapi tidak terlalu imbalanced
    """

    plt.figure(figsize=(12, 8))
    sns.heatmap(df.corr(), annot=False, cmap='coolwarm')
    plt.title('Korelasi Antar Fitur')
    # Simpan gambar ke file PNG
    plt.savefig('heatmap_korelasi.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Korelasi antar fitur (heatmap)
    corr = df.corr()

    # Menghitung korelasi antara fitur dan target
    correlation_with_target = df.corr()['diagnosis'].abs().sort_values(ascending=False)

    # Menampilkan korelasi
    print(correlation_with_target)

    """**Insight:**

    - Fitur seperti concave points_worst, perimeter_worst, dan radius_worst memiliki korelasi tinggi dengan target (diagnosis).
    - Beberapa fitur memiliki korelasi rendah (< 0.1), yang mungkin tidak terlalu informatif untuk model.

    ### Cek Outlier
    """

    df_outlier=df.copy()
    df_outlier = df_outlier.drop(columns=['Unnamed: 32', 'id'], errors='ignore')
    numeric_df = df_outlier.select_dtypes(include=['float64', 'int64'])

    for column in numeric_df.columns:
        plt.figure(figsize=(6, 4))
        sns.boxplot(x=numeric_df[column])
        plt.title(f'Boxplot untuk {column}')
        plt.show()

    """# **5. Data Preprocessing**

    Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.

    Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.

    Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:
    1. Menghapus atau Menangani Data Kosong (Missing Values)
    2. Menghapus Data Duplikat
    3. Normalisasi atau Standarisasi Fitur
    4. Deteksi dan Penanganan Outlier
    5. Encoding Data Kategorikal
    6. Binning (Pengelompokan Data)

    Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur.
    """

    X = df.drop('diagnosis', axis=1)
    y = df['diagnosis']

    """**Insight:**

    - Memisahkan fitur dan target

    ### Handling Missing Values
    """

    df.drop(columns=['Unnamed: 32', 'id'], inplace=True)

    """**Insight:**

    - Melakukan tindakan drop untuk kolom yang tidak relevan seperti Id dan data yang kosong (Unnamed: 32)

    ### Menghapus Data Duplikat

    **Insight:**

    - Karena tidak ada data duplikat dalam dataset, maka tidak diperlukan penanganan untuk data duplikat. Proses analisis dapat dilanjutkan ke tahap berikutnya.

    ### Split Data
    """

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    """**Insight:**
    - Pembagian data train-test untuk memastikan model dievaluasi pada data yang belum pernah dilihat.
    - Data dibagi dengan rasio 80:20 (train:test), dengan stratify untuk menjaga proporsi kelas.

    ### Standarisasi Fitur
    """

    # Standarisasi fitur
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    """**Insight:**

    - Standarisasi mengubah fitur ke skala yang sama (mean=0, std=1).
    - Standarisasi diperlukan karena beberapa algoritma (seperti SVM) sensitif terhadap skala fitur.

    ### Penanganan Outlier

    **Insight:**

    - Penanganan outlier tidak dilakukan pada dataset ini. Alasannya, outlier yang ditemukan pada fitur seperti area_mean dan perimeter_worst mencerminkan kasus nyata, seperti tumor dengan ukuran ekstrem, yang penting untuk membedakan antara diagnosis malignan dan benign. Selain itu, dengan ukuran dataset yang kecil (569 baris), menghapus atau memodifikasi outlier dapat mengurangi informasi berharga dan memengaruhi performa model, terutama karena dataset ini digunakan untuk keperluan medis di mana kasus ekstrem memiliki makna klinis
    """